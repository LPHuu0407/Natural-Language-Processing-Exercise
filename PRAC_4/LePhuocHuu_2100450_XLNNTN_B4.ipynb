{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XDA2qFv7fkw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok_zxeTT7OXH"
      },
      "source": [
        "1. CÁC BƯỚC CƠ BẢN ĐỂ XÂY DỰNG MÔ HÌNH, IMPORT CÁC THỰ VIỆN VÀ MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiXJyKUH8B0r"
      },
      "outputs": [],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/data_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHxxBIrx8EjB"
      },
      "outputs": [],
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "import gensim\n",
        "import os\n",
        "import pickle\n",
        "import codecs\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import regex\n",
        "import pandas as pd\n",
        "from tokenize import group\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmtA5HBY8WK5"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/data_train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxlrlGAB7tGO"
      },
      "source": [
        "2. TIỀN XỬ LÝ DỮ LIỆU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6soW4NW28ZhS"
      },
      "outputs": [],
      "source": [
        "def remove_html(txt):\n",
        "  return regex.sub(r'<[^>]*>', '', txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopword = set()\n",
        "with open('/content/drive/MyDrive/vietnamese-stopwords.txt', 'r', encoding='utf-8') as _fp:\n",
        "  word = _fp.readlines()\n",
        "stopword = [n.replace('\\n', '') for n in word]\n",
        "def remove_stopwords(line):\n",
        "  words = []\n",
        "  for word in line.strip().split():\n",
        "    if word not in stopword:\n",
        "      words.append(word)\n",
        "  return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_preprocess(document):\n",
        "   document = remove_html(document)\n",
        "   lemmatizer = WordNetLemmatizer()\n",
        "   document = ' '.join([lemmatizer.lemmatize(word) for word in document.split()])\n",
        "   document = ViTokenizer.tokenize(document)\n",
        "   document = document.lower()\n",
        "   document = regex.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', '', document)\n",
        "   document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "   document = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), document, flags=re.IGNORECASE)\n",
        "   return document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. TẢI DỮ LIỆU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(directory_path, label):\n",
        "    data_frames = []\n",
        "    file_list = os.listdir(directory_path)\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = file.read()\n",
        "            data = text_preprocess(data)\n",
        "            data = remove_stopwords(data)\n",
        "            df = pd.DataFrame({'text': [data], 'label': [label]})\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "pos_path = '/content/data_train/train/pos'\n",
        "neg_path = '/content/data_train/train/neg'\n",
        "pos_data = load_data(pos_path, 1) \n",
        "neg_data = load_data(neg_path, 0)\n",
        "traindata = pd.concat([pos_data, neg_data], ignore_index=True)\n",
        "traindata.to_csv('/content/traindata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(directory_path, label):\n",
        "    data_frames = []\n",
        "    file_list = os.listdir(directory_path)\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            data = file.read()\n",
        "            data = text_preprocess(data)\n",
        "            data = remove_stopwords(data)\n",
        "            df = pd.DataFrame({'text': [data], 'label': [label]})\n",
        "            data_frames.append(df)\n",
        "    return pd.concat(data_frames, ignore_index=True)\n",
        "pos_path = '/content/data_train/test/pos'\n",
        "neg_path = '/content/data_train/test/neg'\n",
        "pos_data = load_data(pos_path, 1)\n",
        "neg_data = load_data(neg_path, 0)\n",
        "testdata = pd.concat([pos_data, neg_data], ignore_index=True)\n",
        "testdata.to_csv('/content/testdata.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LixfkC7W72oq"
      },
      "source": [
        "4. TRÍCH XUẤT ĐẶC TRƯNG TỪ DỮ LIỆU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "traindata = pd.read_csv('traindata.csv')\n",
        "testdata = pd.read_csv('testdata.csv')\n",
        "X_train = traindata['text']\n",
        "y_train = traindata['label']\n",
        "X_test = testdata['text']\n",
        "y_test = testdata['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
        "tfidf_vect.fit(X_train)\n",
        "X_train_tfidf = tfidf_vect.transform(X_train)\n",
        "X_test_tfidf = tfidf_vect.transform(X_test)\n",
        "svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "svd.fit(X_train_tfidf)\n",
        "X_train_tfidf_svd = svd.transform(X_train_tfidf)\n",
        "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APAisVUK76OX"
      },
      "source": [
        "5. MÔ HÌNH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.1 KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2 SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3 NAIVE BAYES"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
