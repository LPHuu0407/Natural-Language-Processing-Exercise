{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SECTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lphuu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus example: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.', 'the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan']\n",
      "\n",
      "\n",
      "Vocab example: ['muzzle', 'vitriolic', 'autocracies', 'most', 'abernathys', 'includes', 'motley', 'rationing', 'analeptic', 'others', 'pap', 'nor', 'pinnacle', 'respond', '2:05.2', 'undertakings', 'desolation', 'equivalent-choice', 'subsidiaries', 'zeroed', 'empowering', 'underlay', 'dewey', 'mcdaniel', 'treadmill', 'reprisal', \"orleans'\", 'hockett', 'vaulting', 'overpowered', 'sherry', 'demure', 'storing', 'recipes', 'annunciated', 'feast', 'utilitarian', 'working', 'lowest', 'voroshilov', 'scions', 'pencil-pusher', 'shea', 'renown', 'risking', 'canoe', 'desk', 'palm-studded', 'taxation', 'snyder']\n"
     ]
    }
   ],
   "source": [
    "#Loading the corpus\n",
    "corpus = brown.words()\n",
    "#Case folding and getting vocab\n",
    "lower_case_corpus = [w.lower() for w in corpus]\n",
    "vocab = set(lower_case_corpus)\n",
    "\n",
    "print(f'Corpus example: {str(lower_case_corpus[:100])}\\n\\n')\n",
    "print(f'Vocab example: {str(list(vocab)[:50])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example, count for bigram ('the', 'king') is: 51\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = {}\n",
    "trigram_counts = {}\n",
    "#Trượt qua corpus để có được số lượng bigram và trigram\n",
    "for i in range(len(lower_case_corpus) - 2):\n",
    "    # Tạo bigram và trigram\n",
    "    bigram = (lower_case_corpus[i], lower_case_corpus[i+1])\n",
    "    trigram = (lower_case_corpus[i], lower_case_corpus[i+1], lower_case_corpus[i+2])\n",
    "    # Theo dõi số lượng bigram\n",
    "    if bigram in bigram_counts.keys():\n",
    "        bigram_counts[bigram] += 1\n",
    "    else:\n",
    "        bigram_counts[bigram] = 1\n",
    "    # Theo dõi số lượng trigram\n",
    "    if trigram in trigram_counts.keys():\n",
    "        trigram_counts[trigram] += 1\n",
    "    else:\n",
    "        trigram_counts[trigram] = 1\n",
    "print(f\"Example, count for bigram ('the', 'king') is: {str(bigram_counts[('the', 'king')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lấy câu làm đầu vào và gợi ý các từ có thể xuất hiện trong câu\n",
    "def suggest_next_word(input_, bigram_counts, trigram_counts, vocab):\n",
    "    # Tokenize câu đầu vào và chuyển thành chữ thường | lấy bigram cuối cùng, bigram này sẽ được đề xuất cho từ tiếp theo\n",
    "    tokenized_input = word_tokenize(input_.lower())\n",
    "    last_bigram = tokenized_input[-2:]\n",
    "    # Tính xác suất cho mỗi từ trong từ vựng\n",
    "    vocab_probabilities = {}\n",
    "    for vocab_word in vocab:\n",
    "        test_trigram = (last_bigram[0], last_bigram[1], vocab_word)\n",
    "        test_bigram = (last_bigram[0], last_bigram[1])\n",
    "\n",
    "        test_trigram_count = trigram_counts.get(test_trigram, 0)\n",
    "        test_bigram_count = bigram_counts.get(test_bigram, 0)\n",
    "\n",
    "        probability = test_trigram_count / test_bigram_count\n",
    "        vocab_probabilities[vocab_word] = probability\n",
    "    top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    return top_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('james', 0.17647058823529413),\n",
       " ('of', 0.1568627450980392),\n",
       " ('arthur', 0.11764705882352941)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word(\"I am the king\", bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.3333333333333333),\n",
       " ('hearts', 0.16666666666666666),\n",
       " ('spain', 0.08333333333333333)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word(\"I am the king of\", bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.26666666666666666), ('and', 0.26666666666666666), (',', 0.2)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word(\"I am the king of france\", bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.2), ('germany', 0.13333333333333333), ('all', 0.06666666666666667)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word(\"I am the king of france and\", bigram_counts, trigram_counts, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
